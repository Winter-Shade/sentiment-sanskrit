{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Saamayik dataset: 43493 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a63ed91cf72403d993d37b1811f02a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling Saamayik:   0%|          | 0/43493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Saamayik labeled DataFrame, shape: (43493, 3)\n",
      "Saved to data/processed/saamayik_labeled.parquet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "OUT_PATH = \"data/processed/saamayik_labeled.parquet\"\n",
    "\n",
    "# Load Saamayik (train split contains translation objects)\n",
    "ds = load_dataset(\"acomquest/Saamayik\")\n",
    "raw = ds['train']\n",
    "\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device_id\n",
    ")\n",
    "\n",
    "def map_sentiment_label(label_str):\n",
    "    if label_str.lower() == 'negative':\n",
    "        return 0\n",
    "    elif label_str.lower() == 'neutral':\n",
    "        return 1\n",
    "    elif label_str.lower() == 'positive':\n",
    "        return 2\n",
    "    else:\n",
    "        # Some models return 'LABEL_0' style; normalize common cases\n",
    "        s = str(label_str).lower()\n",
    "        if 'neg' in s:\n",
    "            return 0\n",
    "        if 'neu' in s:\n",
    "            return 1\n",
    "        if 'pos' in s:\n",
    "            return 2\n",
    "        raise ValueError(f\"Unknown sentiment label: {label_str}\")\n",
    "\n",
    "texts_en = []\n",
    "texts_sa = []\n",
    "labels = []\n",
    "\n",
    "print(f\"Processing Saamayik dataset: {len(raw)} examples\")\n",
    "for ex in tqdm(raw, desc=\"Labeling Saamayik\"):\n",
    "    translation = ex.get('translation', None)\n",
    "    if translation is None:\n",
    "        continue\n",
    "    # translation may be a dict or JSON string\n",
    "    if isinstance(translation, str):\n",
    "        try:\n",
    "            trans = json.loads(translation)\n",
    "        except Exception:\n",
    "            # fallback: treat the string as raw english text\n",
    "            trans = {'en': translation, 'sa': ''}\n",
    "    elif isinstance(translation, dict):\n",
    "        trans = translation\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    en_text = (trans.get('en') or '').strip()\n",
    "    sa_text = (trans.get('sa') or '').strip()\n",
    "    if not en_text or not sa_text:\n",
    "        continue\n",
    "\n",
    "    # classify english text (batch single)\n",
    "    try:\n",
    "        res = sentiment_classifier(en_text)[0]\n",
    "        label = map_sentiment_label(res.get('label'))\n",
    "    except Exception:\n",
    "        # if classifier fails on a long example, skip\n",
    "        continue\n",
    "\n",
    "    texts_en.append(en_text)\n",
    "    texts_sa.append(sa_text)\n",
    "    labels.append(label)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text_sanskrit\": texts_sa,\n",
    "    \"text_english\": texts_en,\n",
    "    \"label\": labels\n",
    "})\n",
    "print(\"Constructed Saamayik labeled DataFrame, shape:\", df.shape)\n",
    "df.to_parquet(OUT_PATH, index=False)\n",
    "print(\"Saved to\", OUT_PATH)\n",
    "\n",
    "saamayik_labeled = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting IndicTransToolkit\n",
      "  Downloading indictranstoolkit-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting cython (from IndicTransToolkit)\n",
      "  Downloading cython-3.2.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting sacremoses (from IndicTransToolkit)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from IndicTransToolkit) (4.57.3)\n",
      "Collecting sacrebleu (from IndicTransToolkit)\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting indic-nlp-library-itt (from IndicTransToolkit)\n",
      "  Downloading indic_nlp_library_itt-0.1.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting morfessor (from indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from indic-nlp-library-itt->IndicTransToolkit) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from indic-nlp-library-itt->IndicTransToolkit) (2.1.4)\n",
      "Collecting sphinx-argparse (from indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->indic-nlp-library-itt->IndicTransToolkit) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-itt->IndicTransToolkit) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu->IndicTransToolkit)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: regex in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacrebleu->IndicTransToolkit) (2025.11.3)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu->IndicTransToolkit)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu->IndicTransToolkit)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu->IndicTransToolkit)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacremoses->IndicTransToolkit) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacremoses->IndicTransToolkit) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacremoses->IndicTransToolkit) (4.67.1)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinx-8.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading docutils-0.22.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.1.6)\n",
      "Requirement already satisfied: Pygments>=2.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.19.2)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: babel>=2.13 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.17.0)\n",
      "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests>=2.30.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.32.5)\n",
      "Collecting roman-numerals-py>=1.0.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading roman_numerals_py-3.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=23.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (25.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-itt->IndicTransToolkit) (2025.11.12)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library-itt->IndicTransToolkit)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->IndicTransToolkit) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->IndicTransToolkit) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->IndicTransToolkit) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->IndicTransToolkit) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->IndicTransToolkit) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->IndicTransToolkit) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->IndicTransToolkit) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->IndicTransToolkit) (1.2.0)\n",
      "Downloading indictranstoolkit-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (546 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.3/546.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cython-3.2.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading indic_nlp_library_itt-0.1.1-py3-none-any.whl (53 kB)\n",
      "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sphinx-8.2.3-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading roman_numerals_py-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Installing collected packages: morfessor, tabulate, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, sacremoses, roman-numerals-py, portalocker, lxml, imagesize, docutils, cython, colorama, alabaster, sphinx, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library-itt, IndicTransToolkit\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [IndicTransToolkit]IndicTransToolkit]help]\n",
      "\u001b[1A\u001b[2KSuccessfully installed IndicTransToolkit-1.1.1 alabaster-1.0.0 colorama-0.4.6 cython-3.2.1 docutils-0.21.2 imagesize-1.4.1 indic-nlp-library-itt-0.1.1 lxml-6.0.2 morfessor-2.0.6 portalocker-3.2.0 roman-numerals-py-3.1.0 sacrebleu-2.5.1 sacremoses-0.1.1 snowballstemmer-3.0.1 sphinx-8.2.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub -q\n",
    "!pip install IndicTransToolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"hf_T****\")  #use your token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded IMDb:\n",
      "Negative samples: 12500\n",
      "Positive samples: 12500\n",
      "Translating NEGATIVE examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72878b75cc39497dba16be19b59ae074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Neg → Sanskrit:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating POSITIVE examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70d6fdac7034f92ad025f3629d7b6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pos → Sanskrit:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb augmented shape: (25000, 3)\n",
      "Saved to data/processed/imdb_augmented.parquet\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "OUT_PATH_IMDB = \"data/processed/imdb_augmented.parquet\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "imdb = load_dataset(\"imdb\")[\"train\"]\n",
    "\n",
    "# Filter neg/pos\n",
    "neg = [ex[\"text\"] for ex in imdb if ex[\"label\"] == 0]\n",
    "pos = [ex[\"text\"] for ex in imdb if ex[\"label\"] == 1]\n",
    "\n",
    "random.shuffle(neg)\n",
    "random.shuffle(pos)\n",
    "\n",
    "neg_en = neg[:15000]\n",
    "pos_en = pos[:15000]\n",
    "\n",
    "print(\"Loaded IMDb:\")\n",
    "print(\"Negative samples:\", len(neg_en))\n",
    "print(\"Positive samples:\", len(pos_en))\n",
    "\n",
    "# Load IndicTrans2 EN->SAN model\n",
    "model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer_mt = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model_mt = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"san_Deva\"\n",
    "\n",
    "def translate_batch(texts):\n",
    "    prepped = ip.preprocess_batch(texts, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    inputs = tokenizer_mt(\n",
    "        prepped,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model_mt.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            use_cache=True\n",
    "        )\n",
    "    decoded = tokenizer_mt.batch_decode(\n",
    "        out,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    return ip.postprocess_batch(decoded, lang=tgt_lang)\n",
    "\n",
    "def batched_translate(texts, batch_size=16, desc=\"Translating\"):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            sa = translate_batch(batch)\n",
    "        except Exception:\n",
    "            # If a batch fails, handle individually\n",
    "            sa = []\n",
    "            for t in batch:\n",
    "                try:\n",
    "                    sa.append(translate_batch([t])[0])\n",
    "                except Exception:\n",
    "                    sa.append(\"\")\n",
    "        results.extend(sa)\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Translating NEGATIVE examples...\")\n",
    "neg_sa = batched_translate(neg_en, desc=\"Neg → Sanskrit\")\n",
    "\n",
    "print(\"Translating POSITIVE examples...\")\n",
    "pos_sa = batched_translate(pos_en, desc=\"Pos → Sanskrit\")\n",
    "\n",
    "# Build final augmented dataset\n",
    "texts_sanskrit = neg_sa + pos_sa\n",
    "texts_english = neg_en + pos_en\n",
    "labels = [0] * len(neg_sa) + [2] * len(pos_sa)\n",
    "\n",
    "df_imdb = pd.DataFrame({\n",
    "    \"text_sanskrit\": texts_sanskrit,\n",
    "    \"text_english\": texts_english,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "print(\"IMDb augmented shape:\", df_imdb.shape)\n",
    "\n",
    "df_imdb.to_parquet(OUT_PATH_IMDB, index=False)\n",
    "print(\"Saved to\", OUT_PATH_IMDB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (68493, 3)\n",
      "label\n",
      "1    36246\n",
      "2    17342\n",
      "0    14905\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Before balancing:\n",
      "neg: 14905 pos: 17342 neu: 36246\n",
      "\n",
      "After balancing:\n",
      "label\n",
      "2    12500\n",
      "0    12500\n",
      "1    12500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved final dataset to: data/processed/final_clsa_dataset.parquet\n",
      "Final dataset shape: (37500, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sa_path = \"data/processed/saamayik_labeled.parquet\"\n",
    "imdb_path = \"data/processed/imdb_augmented.parquet\"\n",
    "out_path = \"data/processed/final_clsa_dataset.parquet\"\n",
    "\n",
    "df_sa = pd.read_parquet(sa_path)\n",
    "df_imdb = pd.read_parquet(imdb_path)\n",
    "\n",
    "df = pd.concat([df_sa, df_imdb], ignore_index=True)\n",
    "df = df.dropna(subset=[\"text_sanskrit\", \"text_english\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Combined shape:\", df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Split per class\n",
    "df_neg = df[df.label == 0]\n",
    "df_neu = df[df.label == 1]\n",
    "df_pos = df[df.label == 2]\n",
    "\n",
    "print(\"\\nBefore balancing:\")\n",
    "print(\"neg:\", len(df_neg), \"pos:\", len(df_pos), \"neu:\", len(df_neu))\n",
    "\n",
    "TARGET = 12500\n",
    "\n",
    "df_neg_bal = df_neg.sample(n=TARGET, random_state=42) if len(df_neg) >= TARGET else df_neg\n",
    "df_pos_bal = df_pos.sample(n=TARGET, random_state=42) if len(df_pos) >= TARGET else df_pos\n",
    "df_neu_bal = df_neu.sample(n=TARGET, random_state=42) if len(df_neu) >= TARGET else df_neu\n",
    "\n",
    "df_balanced = pd.concat([df_neg_bal, df_pos_bal, df_neu_bal], ignore_index=True)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAfter balancing:\")\n",
    "print(df_balanced[\"label\"].value_counts())\n",
    "\n",
    "# Train/Val split\n",
    "train_df, val_df = train_test_split(\n",
    "    df_balanced,\n",
    "    test_size=0.2,\n",
    "    stratify=df_balanced[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_df = pd.concat([\n",
    "    train_df.assign(split=\"train\"),\n",
    "    val_df.assign(split=\"validation\")\n",
    "])\n",
    "\n",
    "final_df.to_parquet(out_path, index=False)\n",
    "print(\"\\nSaved final dataset to:\", out_path)\n",
    "print(\"Final dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset: (37500, 4)\n",
      "label\n",
      "1    12500\n",
      "0    12500\n",
      "2    12500\n",
      "Name: count, dtype: int64\n",
      "Train: (30000, 4)\n",
      "Validation: (3750, 4)\n",
      "Test: (3750, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5879f648edad40c0ab60726c3c45bc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b950ae83c7d48a5abb0b3167ffa95f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9381bfdbdbb4bd8babc6429d61572c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_sanskrit', 'text_english', 'label', 'split'],\n",
      "        num_rows: 30000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_sanskrit', 'text_english', 'label', 'split'],\n",
      "        num_rows: 3750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_sanskrit', 'text_english', 'label', 'split'],\n",
      "        num_rows: 3750\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd470fb0f3ab4bf684dab0a6613dde2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da98bda444eb40e099b8f5b276724ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a289e1bc824102955125ba9c0a62eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict\n",
    "\n",
    "FINAL_DATA_PATH = \"data/processed/final_clsa_dataset.parquet\"\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "df = pd.read_parquet(FINAL_DATA_PATH)\n",
    "print(\"Balanced dataset:\", df.shape)\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,         \n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,        \n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Validation:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)\n",
    "\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"] = \"validation\"\n",
    "test_df[\"split\"] = \"test\"\n",
    "\n",
    "df_final = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df_final)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": hf_dataset.filter(lambda x: x[\"split\"] == \"train\"),\n",
    "    \"validation\": hf_dataset.filter(lambda x: x[\"split\"] == \"validation\"),\n",
    "    \"test\": hf_dataset.filter(lambda x: x[\"split\"] == \"test\"),\n",
    "})\n",
    "\n",
    "print(dataset_dict)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text_sanskrit\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_sanskrit\", \"text_english\", \"split\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e9b4d5bfaf4b6ca9170734f9974b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbc2b09f249465e8898ecd5e6797698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee27075519654d21a17a9efbad44327c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2632/49953968.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized CLSA training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5628' max='6566' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5628/6566 44:50 < 07:28, 2.09 it/s, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.335142</td>\n",
       "      <td>0.871467</td>\n",
       "      <td>0.871483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.266980</td>\n",
       "      <td>0.893067</td>\n",
       "      <td>0.893248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.246691</td>\n",
       "      <td>0.909867</td>\n",
       "      <td>0.910048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.260179</td>\n",
       "      <td>0.914133</td>\n",
       "      <td>0.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.273548</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>0.913007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.282002</td>\n",
       "      <td>0.913867</td>\n",
       "      <td>0.914091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "{'train_runtime': 2690.5024, 'train_samples_per_second': 78.052, 'train_steps_per_second': 2.44, 'total_flos': 3.552031139328e+16, 'train_loss': 0.27497159261269694, 'epoch': 6.0}\n",
      "Model saved to: ./clsa_xlmr_optimized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'eval_loss': 0.26017871499061584, 'eval_accuracy': 0.9141333333333334, 'eval_f1_macro': 0.9142996553051465, 'eval_runtime': 10.7661, 'eval_samples_per_second': 348.315, 'eval_steps_per_second': 43.563, 'epoch': 6.0}\n",
      "Test results: {'eval_loss': 0.25333917140960693, 'eval_accuracy': 0.9106666666666666, 'eval_f1_macro': 0.9107841309931008, 'eval_runtime': 11.5217, 'eval_samples_per_second': 325.473, 'eval_steps_per_second': 40.706, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "OUTPUT_DIR = \"./clsa_xlmr_optimized\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_fn(batch):\n",
    "    combined = [\n",
    "        sa + \" </s> \" + en\n",
    "        for sa, en in zip(batch[\"text_sanskrit\"], batch[\"text_english\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        combined,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_sanskrit\", \"text_english\", \"split\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  \n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    fp16=USE_FP16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=200,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting optimized CLSA training...\")\n",
    "train_output = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "print(train_output.metrics)\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Model saved to:\", OUTPUT_DIR)\n",
    "\n",
    "val_results = trainer.evaluate(tokenized_dataset[\"validation\"])\n",
    "print(\"Validation results:\", val_results)\n",
    "\n",
    "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Test results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25333917140960693,\n",
       " 'eval_accuracy': 0.9106666666666666,\n",
       " 'eval_f1_macro': 0.9107841309931008,\n",
       " 'eval_runtime': 11.5217,\n",
       " 'eval_samples_per_second': 325.473,\n",
       " 'eval_steps_per_second': 40.706,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
